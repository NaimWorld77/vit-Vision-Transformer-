# In recent years, the Vision Transformer (ViT) model has gained significant popularity in computer vision, particularly for image classification tasks. This paper provides an overview of the use of ViT in image classification, beginning with an introduction to the implementation process and the fundamental architecture of the ViT model. It then examines and summarizes various image classification approaches, including traditional methods, CNN-based techniques, and those leveraging ViT. Additionally, it presents a comparative analysis of CNNs and ViTs. The paper further explores the potential applications and future advancements of ViT in image classification, highlighting its limitations and discussing possible solutions to address them.
## ViT-based image classification method
In 2017, the Transformer architecture, initially proposed by the Google team to address machine translation tasks, gained widespread use in deep learning. This structure departed from traditional CNN and RNN approaches, utilizing only the Attention mechanism for the entire network. In 2020, the Google team adapted this Transformer architecture for image classification tasks, resulting in the Vision Transformer (ViT) model. This model marked a milestone in applying Transformers to computer vision due to its simplicity, effectiveness, and strong scalability, sparking subsequent research in the field.
As the first Transformer model applied to image classification, ViT does have several limitations. These include large computational demands, a high requirement for training data, a limited number of stacked layers, the inability to encode positional information effectively, and a large number of parameters. To address these challenges, various improvements were proposed. For computational efficiency, Wang et al. introduced the Pyramid Vision Transformer (PVT) model, which reduces the number of tokens through convolution between stages and uses additional convolution kernels for K and V in the attention mechanism, helping to reduce the feature map size. PVT is particularly useful in feature extraction for target detection. Liu et al. proposed the Swin Transformer, which reduces the number of tokens by stitching pixel points within 2x2 ranges between stages, followed by linear transformation. By using local windows and shifting them across layers, Swin reduces computational complexity and enables information exchange across layers.
To address the issue of high training data requirements, Touvron et al. introduced the Data-Efficient Image Transformer (DeiT), which incorporates a Distillation token and utilizes distillation to train on fewer data, such as ImageNet, while achieving state-of-the-art results with fewer resources. Yuan et al. proposed the Convolution-enhanced Image Transformer (CeIT), which replaces large convolutions with multiple smaller convolutions and pooling layers, resulting in better convergence, fewer training iterations, and significantly lower training costs.
In response to the challenge of limited stacked layers, Zhou et al. proposed the DeepViT model, which increases the hidden size in the Re-Attention module to diversify its layers with minimal computational and memory costs. Touvron et al. also proposed the CaiT model, which uses a LayerScale technique to scale the output of the Feed-Forward Network (FFN) and Self-Attention (SA) layers using learnable parameters, allowing for more flexibility in the model. To address the positional encoding issue, the ViT model utilizes learnable absolute position encoding, and the Detection Transformer (DETR), introduced by Carion et al., uses fixed position encoding. Finally, for models with a large number of parameters, the Dynamic Vision Transformer (DVT) model uses only a few 4x4 markers to make accurate predictions, while the Lite Vision Transformer (ViT-Lite), proposed by Yang et al., is a smaller and more compact version of the ViT, making it simpler and more efficient.

# Dataset
It contains about 28K medium quality animal images belonging to 10 categories: dog, cat, horse, spyder, butterfly, chicken, sheep, cow, squirrel, elephant.

![output](https://github.com/user-attachments/assets/a72dfa1a-f29f-4a37-8a80-501537a8bc56)
# Method (Vision Tranformer (ViT) Architecture) 

The Vision Transformer (ViT) was introduced in 2020 as a deep neural network (DNN) architecture specifically designed for image recognition. This innovative model leveraged the transformer architecture, initially developed for natural language processing (NLP), and adapted it for computer vision tasks. The fundamental idea behind ViT is to treat images as sequences of patches, akin to token sequences in NLP. ViT exploits the transformative properties of the transformer design to efficiently process and manage these token sequences, demonstrating its adaptability to tasks beyond its original intent.
The transformer architecture, which forms the backbone of ViT, has proven highly versatile and effective in various applications, including image restoration and object detection. This adaptability highlights the transformer’s broad performance capabilities. ViT’s architecture excels in extracting a comprehensive perspective from input images, capturing both local and global features through tokenization and embedding processes. This enables the model to analyze intricate spatial relationships within the image.
A critical innovation in ViT is its introduction of predefined positional embeddings. These embeddings are additional vectors that encode the specific positions of tokens within a sequence. By incorporating positional information before processing tokens through transformer layers, ViT enhances its ability to determine relative token placements and extract vital spatial details. This integration ensures that the model maintains a coherent understanding of the spatial structure of images, further solidifying its utility in various image-based tasks.
The Vision Transformer (ViT) architecture is fundamentally built upon the Multi-head Self-Attention (MSA) mechanism. This core mechanism endows the model with the remarkable ability to focus on various regions within an image simultaneously. MSA consists of multiple distinct "heads," each capable of independently computing attention. These attention heads are designed to concentrate on diverse segments of the input image, producing independent representations that are ultimately combined to form a comprehensive representation of the image. This simultaneous focus across different parts of the input empowers ViT to capture intricate relationships among various elements of the image, enabling richer feature extraction and enhanced performance in image classification tasks.
Despite its advantages, the adoption of MSA introduces additional complexity to the ViT model. The presence of multiple attention heads and the associated processing steps for aggregating their outputs increase both computational demands and resource requirements. This trade-off between enhanced feature representation and computational cost highlights the sophistication of the ViT architecture. The mathematical formulation of MSA involves computing attention scores that are used to weigh the contributions of different input tokens, enabling the model to prioritize relevant features effectively. This approach underpins ViT's ability to generalize well across a variety of image recognition tasks.

The self-attention mechanism is a cornerstone of transformer architectures, enabling the modeling of interactions and associations across input sequences in predictive tasks. Unlike Convolutional Neural Networks (CNNs), which extract features through localized operations, self-attention integrates insights and characteristics from the entire input sequence. This approach captures both global and local information, fostering a more comprehensive representation of the data.
The operation of self-attention begins with the computation of a scalar product between query and key vectors. The resulting attention scores are then normalized using the SoftMax function, which ensures they sum to one. These normalized scores are applied to the value vectors, yielding an improved output representation. This process allows self-attention to focus on relevant aspects of the input data, prioritizing features based on their significance.
Research by Cordonnier et al. explored the relationship between convolutional layers and the self-attention mechanism. Their findings demonstrated that self-attention is a highly adaptable and versatile mechanism, capable of capturing both localized and broader contextual features. This adaptability distinguishes self-attention from traditional convolutional methods, making it a powerful tool for tasks requiring nuanced feature extraction.
In Vision Transformers (ViTs), self-attention serves as the foundation for processing image patches, enabling the model to achieve superior performance in tasks like image classification. The mechanism's ability to consolidate global and local information underpins its effectiveness and versatility in addressing a wide range of predictive challenges.

![Diagram drawio](https://github.com/user-attachments/assets/3f1c00f3-5ec8-45c6-a648-f176237a689b)

# Experiments Result
The fine-tuned Vision Transformer (ViT) model was utilized to classify 10 classes of animal images. ViT’s key advantage over conventional Convolutional Neural Networks (CNNs) lies in its ability to be directly supervised and trained on large datasets, eliminating the dependency on pre-training through auxiliary tasks. Furthermore, ViT demonstrates its strengths by achieving state-of-the-art performance across diverse image recognition tasks, all while maintaining a significantly lower parameter count than traditional CNN architectures. In view of these remarkable benefits, we fine-tuned the ViT model rigorously for the specific task of animal image classification, yielding excellent results. This highlights ViT’s adaptability and efficiency in learning nuanced features within diverse datasets, even in specialized applications like animal classification.
![reslut output](https://github.com/user-attachments/assets/14991fbf-015b-443a-a7e2-2a2907a0ff30)
After training the fine-tuned Vision Transformer (ViT) model on the 10-class animal image classification dataset, an impressive accuracy of 97.68% was achieved. This result highlights the model's capability to learn complex features and patterns essential for distinguishing between different animal classes. Such high accuracy demonstrates the effectiveness of the ViT architecture, which uses self-attention mechanisms to capture both global and local features within images, outperforming traditional CNNs in this regard.
The fine-tuning process played a pivotal role, enabling the pre-trained ViT to adapt to the specific nuances of the dataset. Additionally, the dataset's quality and diversity likely contributed significantly to the model's success. Proper training strategies, such as data augmentation and optimization techniques, also enhanced the learning process. The high accuracy reflects the model's strength but raises questions about potential overfitting, which needs to be addressed by evaluating its performance on separate validation and test sets.
Further analysis of misclassified samples could reveal areas for improvement, while metrics like precision, recall, and F1-score for each class would provide deeper insights into the model's performance. Overall, the result underscores ViT’s adaptability and efficiency, setting a strong foundation for further exploration and potential real-world applications.

# Comparison With SOTA model
The ViT model’s advanced global feature extraction capability, leveraging the self-attention mechanism, proved instrumental in achieving better performance. While MobileNetV2 achieved an accuracy of 90.61% and VGG16 recorded 89.91%, the ViT model significantly surpassed these benchmarks. The improved performance of ViT can be attributed to its ability to capture complex patterns and relationships in the image data more comprehensively than conventional CNNs, which primarily focus on local features.
In addition to accuracy, the ViT model excelled in precision, recall, and F1-score, further confirming its effectiveness in handling the nuances of the dataset. These metrics reflect the model’s balanced performance across all classes, ensuring fewer misclassifications and better generalization. The comparative analysis underscores the transformative potential of ViT, particularly for datasets where detailed global context and intricate feature representation are crucial for accurate classification. This further validates the suitability of the ViT model as the optimal choice for the 10-class animal image classification task.
![image](https://github.com/user-attachments/assets/3fecd1c3-6707-451a-aad4-a68f416021fa)
